{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4S7uZtFVMcLl"
   },
   "source": [
    "As mentioned in anotehr file, i cannot upload my faiss index to drive\n",
    "\n",
    "FAISS indexes contain native C++ state and cannot be reliably serialized with pickle. Indexes hebce, are rebuilt from saved embeddings instead of being loaded from disk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OIFapyRIGSQx"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "PATH_TO_CHUNKS = '/content/drive/MyDrive/colab_stuff/court_hearings_rag/chunk_text.pkl'\n",
    "with open(PATH_TO_CHUNKS, 'rb') as f:\n",
    "    metadata = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xxziEtAcM0Au"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "embedding_bns = np.load('/content/drive/MyDrive/colab_stuff/court_hearings_rag/bns_embed.npy')\n",
    "embedding_apex = np.load('/content/drive/MyDrive/colab_stuff/court_hearings_rag/apex_embed.npy')\n",
    "embedding_high = np.load('/content/drive/MyDrive/colab_stuff/court_hearings_rag/high_embed.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v92smeFr0uNp"
   },
   "source": [
    "again reminder, faiss cpu us used because model is going to be too big to leave any space in GPU. Either way, faiss is quite old thing, it was designed around cpu, its fine, its fast, its default so much so i need to manually make gpu element and copy index there faiss.index_cpu_to_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ucLt1r2ylKN6",
    "outputId": "5b3fa187-728f-427a-ec96-3240000c5c1b"
   },
   "outputs": [],
   "source": [
    "!pip install faiss-cpu\n",
    "import faiss\n",
    "\n",
    "dim = embedding_bns.shape[1]\n",
    "\n",
    "index_bns = faiss.IndexFlatIP(dim)\n",
    "index_apex = faiss.IndexFlatIP(dim)\n",
    "index_high = faiss.IndexFlatIP(dim)\n",
    "\n",
    "index_bns.add(embedding_bns)\n",
    "index_apex.add(embedding_apex)\n",
    "index_high.add(embedding_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PB_uIlSmPmTl",
    "outputId": "6e6938ac-a706-4b5f-9bc5-97ce3569cc19"
   },
   "outputs": [],
   "source": [
    "# validating sizes just confirming its same accross file\n",
    "print(f'size of bns metadata :: {len(metadata[0])} size of index :: {len(embedding_bns)}')\n",
    "print(f'size of apex metadata :: {len(metadata[1])} size of index :: {len(embedding_apex)}')\n",
    "print(f'size of high metadata :: {len(metadata[2])} size of index :: {len(embedding_high)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5NpGcmeHlK9n",
    "outputId": "6fd1e37b-c70d-410b-d7c0-f2ffdd8f69d6"
   },
   "outputs": [],
   "source": [
    "import sentence_transformers\n",
    "model_faiss = sentence_transformers.SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "def search_info(query, model, metadata, top_k=5):\n",
    "    # first use the same sentance embedding model to make query database compatible\n",
    "    query_vector = model.encode(query, normalize_embeddings=True, convert_to_numpy=True)\n",
    "\n",
    "    sim_1, idx_1 = index_bns.search(query_vector, top_k)\n",
    "    sim_2, idx_2 = index_apex.search(query_vector, top_k)\n",
    "    sim_3, idx_3 = index_high.search(query_vector, top_k )\n",
    "\n",
    "    bns, apex, high = metadata\n",
    "    print(len(query))\n",
    "    for q in range(len(query)):\n",
    "        for i in range(top_k):\n",
    "            print(f'{i+1}th most similar to query in bns with similarity :: {sim_1[q][i]} is :: \\n{bns[idx_1[q][i]]}')\n",
    "            print(f'{i+1}th most similar to query in apex with similarity :: {sim_2[q][i]} is :: \\n{apex[idx_2[q][i]]}')\n",
    "            print(f'{i+1}th most similar to query in high with similarity :: {sim_3[q][i]} is :: \\n{high[idx_3[q][i]]}')\n",
    "\n",
    "query = [\n",
    "    \"What constitutes giving false evidence in a judicial proceeding under criminal law?\",\n",
    "    \"What is the punishment for using fabricated or false evidence in a criminal case?\"\n",
    "  ]\n",
    "search_info(query, model_faiss, metadata, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5wtVoDYl8DwN"
   },
   "source": [
    "Well it seems colab refuses to believe i have bitsandbytes until i restart kernel, so do that after installing them.\n",
    "\n",
    "BitsAndBytes is responsible for quantization. Model can and does fit in gpu memory even without quantization, but when dealing with large number of documents for retrieval, it gives OOM error.\n",
    "\n",
    "So i quantized for it. Though i since simplified(considerably dumbed down and reduced) total documents retrievals for generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FQII6kk8lRmu"
   },
   "outputs": [],
   "source": [
    "!pip install -U transformers bitsandbytes\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uZT4ckcf8u28"
   },
   "source": [
    "Well but bear in mind mistral was trained to be helpful to user, to it talks a lot, can't exactly expect it to shut up as we want, even if it does it still talks sometimes cuz query it is going to get are law/crime related so it tries to clear itself by giving some reasoning or advice, so just accept it. Its unexpectedly time consuming to write prompt to get it to do what i want, no more, no less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 95,
     "referenced_widgets": [
      "7e105b6086b34993bb6e876c43df57d4",
      "63e4b7f2df3741539bc5e0351abb6f35",
      "4a033513316e4bf7b71dbaddf348ea8a",
      "21e26ed58ad04bb5b7cb19f22687153b",
      "30fa27c83e3d4947bdaa500775028adb",
      "36f1d5b2105148dbb10f86421ced492f",
      "2a6574d0cffd41df94960582fb7e59ad",
      "5b21ba6fe1a54bbeac9cddd11b440aff",
      "d837a00b03154095bcd2a4528d9a8585",
      "e87a642740cf4ee69394ac399ed642f6",
      "4e2605a16f924bd68a7c166cff6d5f20"
     ]
    },
    "id": "LwHOSheqlePM",
    "outputId": "b9640e5b-1df8-470f-cba0-a559b23280fe"
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\" # automatically does cpu/gpu\n",
    ")\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    use_fast=True # use rust backend when available\n",
    ")\n",
    "model.eval() # it is all pytorch afterall.\n",
    "print(model.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Um6K5HFWh81W"
   },
   "source": [
    "Well i tried with rewriting prompt in an attmpt to improve retrieval, but in most cases the rewritten prompt ends up getting even worse Similarity score than if i sent user query to faiss directly. So that is what i am gonna do, no rewriting.\n",
    "\n",
    "The reason for this, what i believe, is mistral trying to be overly helpful. It rewrites the prompt but the rewritten one(s) usually sound much much more formal with a very formal english. And i can't seem to be able to stop it from being overly helpful, even with few example prompts given, it does improve much more(few shot learning) but not for all types of question(s) or all type of framing i expect it to handle. I think it cannot be tuned very much, if i was to give an analogy, i need tuning in scale of milimeter and mistral works in realm of centimeter(s), either over or under following the instruction(s).\n",
    "\n",
    "Though one place where it did worked phenomenal was when user were to share his/her story instead of asking question directly, maybe because user isn't able to pin point as to what exactly is it that they need to ask. It was good at summarizing the story to a sentance or two which can be relatively easily searched in faiss with decent score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ywbEL0uXyWQc"
   },
   "outputs": [],
   "source": [
    "from enum import IntEnum\n",
    "\n",
    "class Document(IntEnum):\n",
    "    BNS = 0\n",
    "    APEX = 1\n",
    "    HIGH = 2\n",
    "    ALL = 3\n",
    "\n",
    "index_map = {\n",
    "    Document.BNS: index_bns,\n",
    "    Document.APEX: index_apex,\n",
    "    Document.HIGH: index_high,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7CJfXh2Blk-z",
    "outputId": "1a42885a-c822-4f8a-dfc9-a67b2841f025"
   },
   "outputs": [],
   "source": [
    "def faiss_search(query, model, document, metadata, top_k=2):\n",
    "    '''\n",
    "    apex high or bns in document argument to search only one of them\n",
    "    '''\n",
    "    if isinstance(query, str):\n",
    "        query = [query] # we need list for faiss\n",
    "\n",
    "    query_encoded = model.encode(query, normalize_embeddings=True, convert_to_numpy=True)\n",
    "\n",
    "    documents_retrieved = []\n",
    "    similarity_scores = []\n",
    "\n",
    "    retrieved_similarity, retrieved_document = index_map[document].search(query_encoded, top_k)\n",
    "    for q in range(len(query)):\n",
    "        for k in range(top_k):\n",
    "            similarity_scores.append(retrieved_similarity[q][k])\n",
    "            documents_retrieved.append(f'''\n",
    "                confidence : {retrieved_similarity[q][k]},\n",
    "                source : {metadata[document][retrieved_document[q][k]]['source']},\n",
    "                doc : {metadata[document][retrieved_document[q][k]]['chunk_text']}\n",
    "            ''')\n",
    "    return documents_retrieved, similarity_scores\n",
    "\n",
    "def faiss_search_all(query, model, metadata, top_k=2):\n",
    "    if isinstance(query, str):\n",
    "        query = [query]\n",
    "    query_encoded = model.encode(query, normalize_embeddings=True, convert_to_numpy=True)\n",
    "    documents = []\n",
    "    similarity = []\n",
    "\n",
    "    for bases in Document:\n",
    "        if bases == Document.ALL:\n",
    "            continue\n",
    "        sim, doc = index_map[bases].search(query_encoded, top_k)\n",
    "        for q in range(len(query)):\n",
    "            for k in range(top_k):\n",
    "                similarity.append(sim[q][k])\n",
    "                documents.append(f'''\n",
    "                    confidence : {sim[q][k]},\n",
    "                    source : {metadata[bases][doc[q][k]]['source']},\n",
    "                    doc : {metadata[bases][doc[q][k]]['chunk_text']}\n",
    "                ''')\n",
    "    return documents, similarity\n",
    "\n",
    "def faiss_search_all_top(query, model, metadata, top):\n",
    "    # faisss_search_all but return top k matches\n",
    "    documents, similarity = faiss_search_all(query, model, metadata, top)\n",
    "\n",
    "    return sorted(zip(documents, similarity), reverse=True, key = lambda x : x[1])[:top]\n",
    "\n",
    "question = \"Can i take duck from local pond?\"\n",
    "\n",
    "print(faiss_search(question, model_faiss, Document.APEX, metadata, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MXgTNaidkjH0"
   },
   "source": [
    "The cell below defines some prompt(s) which intend to serve different purposes according to need of user. And several combination(s) of prompts are then encapsulated as function(s) of different name(s) to be called for convinience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RzYet_UXlI69"
   },
   "outputs": [],
   "source": [
    "documents = \"\"\n",
    "prompt_reply_with_docs =  \"\"\"<s>[INST]\n",
    "You are a user-facing chat model working in a law firm.\n",
    "You are given:\n",
    "- A user question\n",
    "- A set of document snippets retrieved from a legal database, each with a relevance score\n",
    "Your task is to answer the user’s question using ONLY the information contained in the provided document snippets.\n",
    "\n",
    "Rules:\n",
    "- Use only facts, principles, or reasoning explicitly present in the provided snippets\n",
    "- Do NOT rely on general legal knowledge or assumptions outside the documents\n",
    "- Do NOT infer or reconstruct missing parts of documents\n",
    "- If the provided documents do not directly address the legal issue raised in the question, apologize briefly and refuse to answer\n",
    "- Do NOT mention the number of documents used\n",
    "- Do NOT mention retrieval, embeddings, or confidence scores\n",
    "\n",
    "Your reply must be:\n",
    "- Concise\n",
    "- Easy to understand for a casual user with no legal background\n",
    "- Directly grounded in the provided documents\n",
    "\n",
    "Question:\n",
    "{user_question}\n",
    "\n",
    "Documents:\n",
    "{documents}\n",
    "[/INST]\"\"\".format(user_question = question, documents = documents)\n",
    "\n",
    "prompt_reply_rephrased = \"\"\"<s>[INST]\n",
    "You are a query rewriting engine for a Retrieval-Augmented Generation (RAG) system.\n",
    "\n",
    "Your job:\n",
    "- Rewrite the user's question into a concise, factual, search-oriented query\n",
    "- Remove conversational language, opinions, and filler\n",
    "- Preserve legal / technical terminology\n",
    "- Do NOT answer the question\n",
    "- Do NOT add new information\n",
    "- Output ONLY the rewritten query\n",
    "\n",
    "Think like a search engine, not a chatbot.\n",
    "\n",
    "Question:\n",
    "{user_question}\n",
    "\n",
    "[/INST]\"\"\".format(user_question = question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MrxivmvelnyN"
   },
   "outputs": [],
   "source": [
    "reply = \"\"\n",
    "documents = faiss_search(reply, model_faiss, Document.HIGH, metadata, 2)\n",
    "prompt = \"\"\"<s>[INST]\n",
    "You are a user-facing chat model working in a law firm.\n",
    "You are given:\n",
    "- A user question\n",
    "- A set of document snippets retrieved from a legal database, each with a relevance score\n",
    "Your task is to answer the user’s question using ONLY the information contained in the provided document snippets.\n",
    "\n",
    "Rules:\n",
    "- Use only facts, principles, or reasoning explicitly present in the provided snippets\n",
    "- Do NOT rely on general legal knowledge or assumptions outside the documents\n",
    "- Do NOT infer or reconstruct missing parts of documents\n",
    "- If the provided documents do not directly address the legal issue raised in the question, apologize briefly and refuse to answer\n",
    "- Do NOT mention anything about existance of documents in your reply\n",
    "- Do NOT mention retrieval, embeddings, or confidence scores\n",
    "\n",
    "Your reply must be:\n",
    "- Concise\n",
    "- Easy to understand for a casual user with no legal background\n",
    "- Directly grounded in the provided documents\n",
    "\n",
    "Question:\n",
    "{user_question}\n",
    "\n",
    "Documents:\n",
    "{documents}\n",
    "[/INST]\"\"\".format(user_question = question, documents = documents)\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=300,\n",
    "        do_sample=False\n",
    "    )\n",
    "\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k9v8wfoClZMZ"
   },
   "outputs": [],
   "source": [
    "def ask_model(question):\n",
    "    prompt = f\"{question}\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=300,\n",
    "            do_sample=False\n",
    "        )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "def ask_with_retrieval(question, document=Document.APEX, top_k=2, cutoff_score=0.0, show_docs=False):\n",
    "    \"\"\"\n",
    "    cutoff_score will later need to be passed to faiss search function :: TODO\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "\n",
    "    if document == Document.ALL:\n",
    "        documents, _ = faiss_search_all(question, model_faiss, metadata, top_k)\n",
    "    else:\n",
    "        documents, _ = faiss_search(question, model_faiss, document, metadata, top_k)\n",
    "\n",
    "    if show_docs:\n",
    "        for entries in documents:\n",
    "            print(entries)\n",
    "\n",
    "    prompt = prompt_reply_with_docs.format(user_question = question, documents = documents)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=300,\n",
    "            do_sample=False\n",
    "        )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "def ask_with_rephrasing(question, document=Document.APEX,top_k=2, cutoff_score=0.45, show_docs=False, compare=False):\n",
    "    documents = []\n",
    "    raw_scores = []\n",
    "    if document == Document.ALL:\n",
    "        documents, raw_scores = faiss_search_all(question, model_faiss, metadata, top_k)\n",
    "    else:\n",
    "        documents, raw_scores = faiss_search(question, model_faiss, document, metadata, top_k)\n",
    "\n",
    "    query = prompt_reply_rephrased.format(user_question = question)\n",
    "    inputs_1 = tokenizer(query, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        output_1 = model.generate(\n",
    "            **inputs_1,\n",
    "            max_new_tokens=300,\n",
    "            do_sample=False\n",
    "        )\n",
    "    rephrased_query = tokenizer.decode(output_1[0], skip_special_tokens=True)\n",
    "    retrieval = []\n",
    "    scores = []\n",
    "    if document == Document.ALL:\n",
    "        retrieval, scores = faiss_search_all(rephrased_query, model_faiss, metadata, top_k)\n",
    "    else:\n",
    "        retrieval, scores = faiss_search(rephrased_query, model_faiss, document, metadata, top_k)\n",
    "    if show_docs:\n",
    "        for doc in retrieval:\n",
    "            print(doc)\n",
    "    if compare:\n",
    "        print(f\"scores of docs fetched by user query on faiss :: {raw_scores}\")\n",
    "        print(f\"scores of docs fetched after rewriting by model :: {scores}\")\n",
    "\n",
    "    prompt = prompt_reply_with_docs.format(user_question = rephrased_query, documents = retrieval)\n",
    "    inputs_2 = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        output_2 = model.generate(\n",
    "            **inputs_2,\n",
    "            max_new_tokens=300,\n",
    "            do_sample=False\n",
    "        )\n",
    "    return tokenizer.decode(output_2[0], skip_special_tokens=True)\n",
    "\n",
    "def get_full_document(id):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l3T208L43kek"
   },
   "outputs": [],
   "source": [
    "\n",
    "ask_with_rephrasing(question, Document.ALL, 2, 0.34, False, True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
