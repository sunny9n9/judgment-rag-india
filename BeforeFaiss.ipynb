{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0yxDBra1WOhU"
   },
   "outputs": [],
   "source": [
    "# [ ----- BASIC INSTALLS AND MODULE LOADING(S) ----- ]\n",
    "!pip install faiss-cpu\n",
    "!pip install sentence-transformers\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import sentence_transformers\n",
    "import pandas\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "TRUE = True\n",
    "FALSE = False\n",
    "FRESH_START = FALSE\n",
    "TEST_RUN = FALSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dQHVSOzE12s9"
   },
   "source": [
    "Well this True False is very confusing, i do true or TRUE cuz cpp/winapi so i defined those; should ctrl f -> replace all TRUE with True and same for false but if i work on it in future, most probably will face this again so im keeping it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_0-fx6BKXyiw"
   },
   "outputs": [],
   "source": [
    "# [ ----- GET BNS DATASET TO BUILD RAG AROUND ----- ]\n",
    "# this cell if for loading kaggle and getting BNS dataset\n",
    "if FRESH_START:\n",
    "    !mkdir -p ~/.kaggle\n",
    "    !mv kaggle.json ~/.kaggle/\n",
    "    !chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "    !kaggle datasets download -d nandr39/bharatiya-nyaya-sanhita-dataset-bns\n",
    "    !unzip bharatiya-nyaya-sanhita-dataset-bns.zip -d bns_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vc56L8rhYOkn"
   },
   "source": [
    "**Since the dataset is so big, I saved one copy in drive for now for, fast upload**\n",
    "\n",
    "SOURCE : https://www.kaggle.com/datasets/adarshsingh0903/legal-dataset-sc-judgments-india-19502024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O4OGIQZ5X2Ql"
   },
   "outputs": [],
   "source": [
    "# [ ----- GET SUPEREME COURT TO BUILD RAG AROUND ----- ]\n",
    "# this cell is for {loading kaggle} and supereme courts' dataset\n",
    "\n",
    "if FRESH_START:\n",
    "    !cp /content/drive/MyDrive/colab_stuff/court_hearings_rag/archive.zip /content/\n",
    "    !unzip archive.zip -d /content/sc_judgments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nyVQyKKKZdB4"
   },
   "source": [
    "The Sections below are all responsible for visiting the supereme courts' dataset(unzipped and script.py deleted) file by file and extracting relevant data, reading pdf(s) and saving them as a file\n",
    "\n",
    "Last time this was run(and first time) it took ~16 hours, so better use the saved, parsed data from drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pMDqkqW8aLkf"
   },
   "source": [
    "I also happened to write a multiprocessing version for this parsing and saving pdf(s) only to realize colab is gonna give me only 1 process to work with anyways. Also i found out multiprocessing is slower in windows than linux(the type i needed which made a process for 1 function, ran it, returned value, died, over and over).\n",
    "\n",
    "I also considered using local machine for this, R5 7535HS, which is afterall, 6C 12T capable processor onnly to realize me using multi core(s) is still somehow slower than what single process of colab and achieve, in terms of speed, hence not pasting that code here. If intereseted, can see it in prev upload, when whole project was single notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lBDaUoJZYuLs"
   },
   "outputs": [],
   "source": [
    "# [ ------ For Saving Supereme courts' dataset cuz its too big to load everytime ----- ]\n",
    "# we save each case data like this for later convinience, called in single core one\n",
    "path_to_json = '/content/apex.jsonl' # advice gotten use json line, advice accepted\n",
    "all_cases = []\n",
    "\n",
    "def save_it(pages, file, file2, id):\n",
    "    temp = {\n",
    "        'id' : id,\n",
    "        'source' : \"Supreme Court\",\n",
    "        'title' : file2,\n",
    "        'text' : pages,\n",
    "        'meta' : {\n",
    "            'year' : file\n",
    "            }\n",
    "    }\n",
    "    with open(path_to_json, \"a\", encoding='utf-8') as f:\n",
    "        f.write(json.dumps(temp, ensure_ascii=False)) # dumps returns string of what dump was supposed to save\n",
    "                                                    # we need to write line by line hence the way of f.write(json.dumps())\n",
    "        f.write(\"\\n\") # this is how we make jsonl mate\n",
    "\n",
    "all_cases.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_b4mTqXBZIST"
   },
   "outputs": [],
   "source": [
    "# [ ----- Responsible for removing headers and footers' of pdf of supereme court judgements ----- ]\n",
    "def _apex_remove_header_footer(text):\n",
    "    HEADER = None\n",
    "    FOOTER = f'Indian Kanoon - '\n",
    "    HEADER, _ = current_case['text'].split('\\n', 1) # in the text region the first line before '\\n' gives us header, so only 1 split needed\n",
    "    cleaned_current_case = []\n",
    "\n",
    "    for line in current_case['text'].split('\\n'):\n",
    "        if line.startswith(HEADER) or line.startswith(FOOTER):\n",
    "            continue\n",
    "        else:\n",
    "            cleaned_current_case.append(line)\n",
    "            # a weird observation, even after loads() my footer of this and header of next page don't have \\n in between\n",
    "            # so like just detecting footers is enough to remove headers as well\n",
    "    return '\\n'.join(cleaned_current_case)  # we split on '\\n' so we join on '\\n\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NYzT3IEQZLqr"
   },
   "outputs": [],
   "source": [
    "# [ ----- For cleaning subtle things bcs pdf has page so words get cut ----- ]\n",
    "# OBSERVATION 1 - if a sentance ends with \\n but no fullstop before it, it was cut midway, remove \\n\n",
    "# OBSERVATION 2 - if a word has -\\n it was cut midway, remove -\\n\n",
    "# OBSERVATION 3 - multiple -- serve some purpose, wait now, remove later\n",
    "def _remove_enter_mid_words(text):\n",
    "    formatted_sentance = []\n",
    "    sentances = text.split('. ')\n",
    "    for sentance in sentances:\n",
    "        sentance = sentance.replace('-\\n', '')\n",
    "        sentance = sentance.replace('\\n', ' ')\n",
    "        formatted_sentance.append(sentance)\n",
    "    return '.\\n'.join(formatted_sentance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zzcyK2shZRw_"
   },
   "outputs": [],
   "source": [
    "'''SINGLE CORE SINGLE CORE SINGLE CORE SINGLE CORE SINGLE CORE SINGLE CORE SINGLE CORE'''\n",
    "import os\n",
    "apex_raw = []\n",
    "\n",
    "if FRESH_START:\n",
    "    import pdfplumber\n",
    "    ROOT = \"/content/sc_judgments/supreme_court_judgments\"\n",
    "    start_id = sum(1 for _ in open(path_to_json, \"r\", encoding=\"utf-8\"))\n",
    "    last_line = None\n",
    "\n",
    "    print(f'starting fro i :: {start_id}')\n",
    "    i = 1\n",
    "    for file in sorted(os.listdir(ROOT)): # order of traversal is weird without sorted\n",
    "        # print(file)\n",
    "        print(f\"\\ncurrently in year :: {file}\\n currently in case :: \", end=' ') # so i don't die waiting\n",
    "        for file2 in sorted(os.listdir(f\"{ROOT}/{file}\")):\n",
    "            if i < start_id:\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "        print(f\"{i}\", sep=' ', end=' ')\n",
    "        try:\n",
    "            with pdfplumber.open(f\"{ROOT}/{file}/{file2}\") as pdf:\n",
    "                pages = []\n",
    "                for page in pdf.pages:\n",
    "                    try:\n",
    "                        text = page.extract_text()\n",
    "                        if text:\n",
    "                            pages.append(text)\n",
    "                            # print(pages)\n",
    "                    except Exception:\n",
    "                        print(f'something wrong with page in pdf :: {i} ')\n",
    "                        with open('/content/errors.jsonl', 'a') as f:\n",
    "                            f.write(json.dumps(f' PAGE PROBLEM :: PDF :: {i} Year :: {file} Title :: {file2}'))\n",
    "                            f.write('\\n')\n",
    "                            f.close()\n",
    "                    # only after pages has been appended with all data\n",
    "                save_it(pages, file, file2, i)\n",
    "                i += 1\n",
    "        except Exception as e: # are these always meant to be used or always realized the need for\n",
    "            print(f'something wrong with pdf :: {i}')\n",
    "            with open('/content/errors.jsonl', 'a') as f:\n",
    "                f.write(json.dumps(f' PDF PROBLEM :: PDF :: {i} Year :: {file} Title :: {file2}'))\n",
    "                f.write('\\n')\n",
    "                f.close()\n",
    "        !cp {path_to_json} /content/drive/MyDrive/colab_stuff/court_hearings_rag/apex.jsonl\n",
    "        print(f'\\n:: copied to drive ::\\n')\n",
    "else:\n",
    "    !cp /content/drive/MyDrive/colab_stuff/court_hearings_rag/apex.jsonl {path_to_json}\n",
    "    with open(path_to_json, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            current_case = json.loads(line)\n",
    "            current_case['text'] = _remove_enter_mid_words(_apex_remove_header_footer(current_case['text']))\n",
    "            apex_raw.append(current_case)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ckOeG4XMb2WP"
   },
   "source": [
    "Source for BNS :: https://www.kaggle.com/datasets/nandr39/bharatiya-nyaya-sanhita-dataset-bns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V5H76woO2lq-"
   },
   "source": [
    "Initially all dataset had custom representation because each dataset unique, but over time i was getting irritated writing functions for each of their handling so i made a cell to unify everyones' representation to be the same after apex dataset was done with. Though i want to remember these things so i keep it/preserve as much of originality/suffering/story as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cgMknaBibq6K"
   },
   "outputs": [],
   "source": [
    "# [LOAD BNS DATASET]\n",
    "SAVE_PATH1 = \"/content/drive/MyDrive/colab_stuff/court_hearings_rag/bns.json\"\n",
    "\n",
    "if FRESH_START:\n",
    "    bns_raw = pandas.read_csv(\"/content/bns_data/bns_sections.csv\")\n",
    "    print(bns_raw.head())\n",
    "    bns = []\n",
    "    for i, entries in bns_raw.iterrows():\n",
    "        temp = {\n",
    "        'id' : i+1,\n",
    "        'chapter': entries['Chapter'],\n",
    "        'section': entries['Section'],\n",
    "        'title': entries['Section _name'],\n",
    "        'text': entries['Description']\n",
    "        }\n",
    "        bns.append(temp)\n",
    "    print(bns[0])\n",
    "\n",
    "    with open(SAVE_PATH1, 'w') as f:\n",
    "        json.dump(bns, f)\n",
    "else:\n",
    "    with open(SAVE_PATH1, 'r') as f:\n",
    "        bns = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P_dp_uJ0b69f"
   },
   "outputs": [],
   "source": [
    "def _clean_bns_data(data):\n",
    "    sentances = data['text'].split('\\r\\n')\n",
    "    data['text'] = '\\n'.join(sentances)\n",
    "    return data\n",
    "\n",
    "for data in bns:\n",
    "    data = _clean_bns_data(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1UKC-5xMbRVL"
   },
   "source": [
    "Get high court and supereme courts' dataset of which i won't use supereme court cuz i have better dataset for that. I will filter only high court cases from this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "orOWMr8DY944"
   },
   "outputs": [],
   "source": [
    "if FRESH_START:\n",
    "    TOKEN = \"\"\n",
    "    dataset_supereme_high_court = datasets.load_dataset(\"opennyaiorg/InJudgements_dataset\", token=TOKEN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ykSno9wd3dz6"
   },
   "source": [
    "This dataset seems like scraped from web containing several html fields so gotta remove them by regex, and it contains decorations in forms of ===== so get rid of that as well\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aaqFRtL7cNII"
   },
   "outputs": [],
   "source": [
    "# [LOAD SUP-HI COURT DATASET]\n",
    "SAVE_PATH2 = \"/content/drive/MyDrive/colab_stuff/court_hearings_rag/high_courts.json\"\n",
    "\n",
    "# [THE TEXT DATA IS TOO NOISY WITH HTML TAGS]\n",
    "def clean_html(text):\n",
    "    import re\n",
    "    text = re.sub(r\"<span[^>]*>.*?</span>\", \" \", text, flags=re.DOTALL) # REMOVE 'EM HTML\n",
    "    text = re.sub(r\"<[^>]+>\", \" \", text) # REMOVE 'EM HTML\n",
    "    text = re.sub(r\"[=\\-_.]{4,}\", \" \", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# print(dataset_supereme_high_court)\n",
    "# print(dataset_supereme_high_court['train'][0])\n",
    "if FRESH_START:\n",
    "    high_courts_raw = dataset_supereme_high_court['train']\n",
    "    # high_courts_raw[0]\n",
    "    # print(high_courts_raw[0].keys())\n",
    "    high_courts = []\n",
    "    for i, cases in enumerate(high_courts_raw):\n",
    "        temp = {\n",
    "            'id' : i+1,\n",
    "            'court_details' : f\"{cases['Court_Type']} | {cases['Case_Type']} | {cases['Court_Name']}\",\n",
    "            'cited_by' : cases['Cited_by'],\n",
    "            'title': cases['Titles'],\n",
    "            'text' : clean_html(cases[\"Text\"])\n",
    "        }\n",
    "        high_courts.append(temp)\n",
    "\n",
    "    print(high_courts[0])\n",
    "    with open(SAVE_PATH2, \"w\") as f:\n",
    "        json.dump(high_courts, f)\n",
    "else:\n",
    "    with open(SAVE_PATH2, \"r\") as f:\n",
    "        high_courts = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MaGEhae6ctY1"
   },
   "outputs": [],
   "source": [
    "# [ ----- COnsider only high courts' decisions from this dataset ----- ]\n",
    "# well let us drop all supereme court cases from here and use it has high court dataset\n",
    "\n",
    "high_courts = [high_court for high_court in high_courts if high_court['court_details'].startswith('High')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mp3GzgDlcyXr"
   },
   "outputs": [],
   "source": [
    "def _make_high_court_paragraphs(data):\n",
    "    cleaned_lines = []\n",
    "    # print(f\":: toatal paragraphs made {len(data['text'].split('\\n\\n'))} :: \")\n",
    "    length = 0\n",
    "    for line in data['text'].split('\\n\\n'):\n",
    "        line = \" \".join(line.split())\n",
    "        cleaned_lines.append(line)\n",
    "        length = max(length, len(line))\n",
    "    # print(f\"longest paragraphs' length is {length}\")\n",
    "    data['text'] = '\\n'.join(cleaned_lines)\n",
    "    return data\n",
    "\n",
    "print(high_courts[0])\n",
    "\n",
    "for cases in high_courts:\n",
    "    cases = _make_high_court_paragraphs(cases)\n",
    "\n",
    "# print(high_courts[0])\n",
    "# print(high_courts[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8rv9-rPd4FjH"
   },
   "source": [
    "Yes, by this time i had experienced too much pain loading datasets so many times having to deal with two different dictionary for 2 dataset that i decided to finally standardize dictionary keys accross datasets' representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KyZDtblUc3J_"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[UNIFYING ALL DATA REPRESENTATIONS]\n",
    "well i see no point in doing this, but apprearently it helps later on figuring what went wrong in\n",
    "RAG design and extending it in future as well.\n",
    "\"\"\"\n",
    "\n",
    "new_bns = []\n",
    "for entries in bns:\n",
    "    temp = {\n",
    "        'id' : f\"BNS_{entries['id']}\",\n",
    "        'source' : \"BNS\",\n",
    "        'title' : entries['title'],\n",
    "        'text' : entries['text'],\n",
    "        'meta' : {\n",
    "            'chapter' : entries['chapter'],\n",
    "            'section' : entries['section']\n",
    "            }\n",
    "    }\n",
    "    new_bns.append(temp)\n",
    "\n",
    "bns = new_bns\n",
    "del new_bns\n",
    "\n",
    "new_court = []\n",
    "for entries in high_courts:\n",
    "    temp = {\n",
    "        'id' : f\"COURTS_{entries['id']}\",\n",
    "        'source' : \"Courts\",\n",
    "        'title' : entries['title'],\n",
    "        'text' : entries['text'],\n",
    "        'meta' : {\n",
    "            'cited_by' : entries['cited_by'],\n",
    "            'court_details' : entries['court_details']\n",
    "            }\n",
    "    }\n",
    "    new_court.append(temp)\n",
    "court = new_court # yeh i changed name.\n",
    "del new_court\n",
    "del high_courts\n",
    "print(court[0])\n",
    "print(\"\\n\\n\")\n",
    "print(bns[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CWDpA2V94ffH"
   },
   "source": [
    "Just checking if things are progressingn as expected-\n",
    "\n",
    "all the keys of dictionary are correct\n",
    "all dataset contains instances of \\n for later chunkning step\n",
    "no useless space or fancy symbol left in text field\n",
    "\n",
    "some problems does exist like .pdf in title or short title in bns but by this point in time i don't have real use of all these metadata so i will ignore them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rs3ncVXkc_RR",
    "outputId": "d48c4ca8-1b82-443a-d351-fd59d52dd9f2"
   },
   "outputs": [],
   "source": [
    "# VISUAL VERIFICATION\n",
    "print(court[0])\n",
    "print(\"\\n\")\n",
    "print(bns[0])\n",
    "print(\"\\n\")\n",
    "print(apex_raw[0])\n",
    "print(\"\\n\\n\")\n",
    "print(court[0]['text'][:7000])\n",
    "print(\"\\n\")\n",
    "print(bns[0]['text'][:700])\n",
    "print(\"\\n\")\n",
    "print(apex_raw[0]['text'][:700])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WF5c8H8urQy7"
   },
   "source": [
    "For testing purposes, i will take a small sample of data to run cells below fast so i don't have to wait hours. This is just to verify the pipeline after this, as they take hours for embedding and chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E-YRkvoErPgF"
   },
   "outputs": [],
   "source": [
    "if TEST_RUN:\n",
    "    apex_raw = apex_raw[:50]\n",
    "    court = court[:50]\n",
    "    bns = bns[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U3OrUO4SdPP9"
   },
   "source": [
    "CHUNKING - the pain and grail of RAG. This is where i expect most improvements to come from. Chunk length is 300 only cuz long chunks was getting lots of different context making it very noisy and it was too long to feed into model(s) later. Rather have small relevant high precision chunks and retrieve appropriate data based on metadata sstored with it, i mwan why even was that stored anyways otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tVYSo3NjOHpx"
   },
   "outputs": [],
   "source": [
    "def split_long_paragraph(paragraph, max_words):\n",
    "    words = paragraph.split()\n",
    "    parts = []\n",
    "    for i in range(0, len(words), max_words):\n",
    "        parts.append(\" \".join(words[i:i + max_words]))\n",
    "    return parts\n",
    "\n",
    "def _label_chunks(chunks, data):\n",
    "    chunks_final = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        temp = {\n",
    "            'chunk_id': f\"{data['source']}_{data['id']}_{i}\",\n",
    "            'chunk_text': \"\\n\".join(chunk),\n",
    "            'parent_id': data['id'],\n",
    "            'source': data['source']\n",
    "        }\n",
    "        chunks_final.append(temp)\n",
    "    return chunks_final\n",
    "\n",
    "def make_chunks(data, tokenizer, para_overlap=1, max_tokens=800, token_overlap=150):\n",
    "\n",
    "    paragraphs = [p.strip() for p in data[\"text\"].split(\"\\n\") if p.strip()]\n",
    "    chunks = []\n",
    "    chunk = []\n",
    "    running_len = 0\n",
    "    expanded_paragraphs = []\n",
    "\n",
    "    # word-based limit derived from token budget\n",
    "    max_words = int(0.7 * max_tokens)\n",
    "\n",
    "    # long para(s) we have a lot of these unfortunately\n",
    "    for p in paragraphs:\n",
    "        p_len = len(tokenizer.encode(p, add_special_tokens=False))\n",
    "\n",
    "        if p_len > max_tokens:\n",
    "            parts = split_long_paragraph(p, max_words)\n",
    "            for part in parts:\n",
    "                part_len = len(tokenizer.encode(part, add_special_tokens=False))\n",
    "                expanded_paragraphs.append((part, part_len))\n",
    "        else:\n",
    "            expanded_paragraphs.append((p, p_len))\n",
    "    # -----------------------------------------------\n",
    "\n",
    "    paragraphs = [p for p, _ in expanded_paragraphs]\n",
    "    para_tokens = [l for _, l in expanded_paragraphs]\n",
    "\n",
    "    for i, paragraph in enumerate(paragraphs):\n",
    "        p_len = para_tokens[i]\n",
    "\n",
    "        # if adding paragraph would overflow\n",
    "        if running_len + p_len > max_tokens:\n",
    "            if chunk:\n",
    "                chunks.append(chunk.copy())\n",
    "\n",
    "            # start new chunk with overlap\n",
    "            chunk = []\n",
    "            running_len = 0\n",
    "\n",
    "            if para_overlap > 0:\n",
    "                for j in range(max(0, i - para_overlap), i):\n",
    "                    overlap_len = para_tokens[j]\n",
    "                    if running_len + overlap_len > token_overlap:\n",
    "                        break\n",
    "                    chunk.append(paragraphs[j])\n",
    "                    running_len += overlap_len\n",
    "\n",
    "        chunk.append(paragraph)\n",
    "        running_len += p_len\n",
    "\n",
    "    if chunk:\n",
    "        chunks.append(chunk.copy())\n",
    "\n",
    "    return _label_chunks(chunks, data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bMb15Ylc5-Ex"
   },
   "source": [
    "this tqdm is godsent, i now know not knowing how long to wait is more painful than having to wait 16 hours for all pdf to be parsed. And i switched from jsons to pickle for my preffered method of storing data cuz iz python aftrall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tU_twGCseuMg"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "model = sentence_transformers.SentenceTransformer('all-MiniLM-L6-v2')\n",
    "tokenizer = model.tokenizer\n",
    "chunks_bns = []\n",
    "chunks_high = []\n",
    "chunks_apex = []\n",
    "if FRESH_START:\n",
    "    print(f':: currently doing BNS :: ')\n",
    "    for cases in tqdm(bns):\n",
    "        labeled_chunk = make_chunks(cases, tokenizer, 0, 300, 0)\n",
    "        chunks_bns += labeled_chunk\n",
    "    with open('/content/bns_chunked.pkl', 'wb') as f:\n",
    "        pickle.dump(chunks_bns, f)\n",
    "        !cp /content/bns_chunked.pkl /content/drive/MyDrive/colab_stuff/court_hearings_rag/bns_chunked.pkl\n",
    "    print(f\" :: currently doing high_courts :: \")\n",
    "\n",
    "    for cases in tqdm(court):\n",
    "        labeled_chunk = make_chunks(cases, tokenizer, 1, 300, 65)\n",
    "        chunks_high += labeled_chunk\n",
    "    with open('/content/high_chunked.pkl', 'wb') as f:\n",
    "        pickle.dump(chunks_high, f)\n",
    "        !cp /content/high_chunked.pkl /content/drive/MyDrive/colab_stuff/court_hearings_rag/high_chunked.pkl\n",
    "    print(f' :: currently doing apex :: ')\n",
    "\n",
    "    for cases in tqdm(apex_raw):\n",
    "        labeled_chunk = make_chunks(cases, tokenizer, 1, 300, 65)\n",
    "        chunks_apex += labeled_chunk\n",
    "    with open('/content/apex_chunked.pkl', 'wb') as f:\n",
    "        pickle.dump(chunks_apex, f)\n",
    "        !cp /content/apex_chunked.pkl /content/drive/MyDrive/colab_stuff/court_hearings_rag/apex_chunked.pkl\n",
    "else:\n",
    "    with open('/content/drive/MyDrive/colab_stuff/court_hearings_rag/apex_chunked.pkl', 'rb') as f:\n",
    "        chunks_apex = pickle.load(f)\n",
    "\n",
    "    with open('/content/drive/MyDrive/colab_stuff/court_hearings_rag/high_chunked.pkl', 'rb') as f:\n",
    "        chunks_high = pickle.load(f)\n",
    "\n",
    "    with open('/content/drive/MyDrive/colab_stuff/court_hearings_rag/bns_chunked.pkl', 'rb') as f:\n",
    "        chunks_bns = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JXGaBWDRfAKS",
    "outputId": "dc9f9394-2f0c-475b-f5a0-1f477063eddc"
   },
   "outputs": [],
   "source": [
    "print(f'{len(chunks_apex)}')\n",
    "print(f'{len(chunks_high)}')\n",
    "print(f'{len(chunks_bns)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YZB4RR_5fPeg"
   },
   "source": [
    "EMBEDDING :: well then we need to embed the chunks thus made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mn9htQB5fLhg"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "make embedding and select model\n",
    "'''\n",
    "model_name = 'all-MiniLM-L6-v2'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = sentence_transformers.SentenceTransformer(model_name, device=device)\n",
    "# remember that faiss is cpu first, need to move everything to gpu manually\n",
    "\n",
    "def embed(model, embedding_name, chunks):\n",
    "    # model = sentence_transformers.SentenceTransformer(model_name, device=device)\n",
    "    path = f'/content/embedding_{embedding_name}.npy'\n",
    "    print(f'currently embedding za {embedding_name}')\n",
    "\n",
    "    if FRESH_START:\n",
    "        # model.to(device) not adviced, do at creation, else things break\n",
    "        chunk_texts = [chunk['chunk_text'] for chunk in chunks]\n",
    "        embeddings = model.encode(\n",
    "            chunk_texts,\n",
    "            batch_size = 64,\n",
    "            convert_to_numpy=True, # remember faiss numpy\n",
    "            show_progress_bar=True,\n",
    "            normalize_embeddings=True,\n",
    "            device=device\n",
    "            )\n",
    "        np.save(path, embeddings)\n",
    "        !cp {path} /content/drive/MyDrive/colab_stuff/court_hearings_rag/{embedding_name}.npy\n",
    "    else:\n",
    "        embeddings = np.load(path)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ayX8QaqNfhpb"
   },
   "outputs": [],
   "source": [
    "if FRESH_START:\n",
    "    embedding_bns = embed(model, 'bns_embed', chunks_bns)\n",
    "    embedding_apex = embed(model, 'apex_embed', chunks_apex)\n",
    "    embedding_high = embed(model, 'high_embed', chunks_high)\n",
    "else:\n",
    "    embedding_bns = np.load('/content/drive/MyDrive/colab_stuff/court_hearings_rag/bns_embed.npy')\n",
    "    embedding_apex = np.load('/content/drive/MyDrive/colab_stuff/court_hearings_rag/apex_embed.npy')\n",
    "    embedding_high = np.load('/content/drive/MyDrive/colab_stuff/court_hearings_rag/high_embed.npy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BIdAX40bf1_-"
   },
   "source": [
    "And finally, we have our FAISS, this thing has soooo much potential, i gotta explore this in much greater depth in future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hR4yybLAfpyd"
   },
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "dim = embedding_bns.shape[1]\n",
    "\n",
    "index_bns = faiss.IndexFlatIP(dim)\n",
    "index_apex = faiss.IndexFlatIP(dim)\n",
    "index_high = faiss.IndexFlatIP(dim)\n",
    "\n",
    "index_bns.add(embedding_bns)\n",
    "index_apex.add(embedding_apex)\n",
    "index_high.add(embedding_high)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y0Xh8SyVL2zq"
   },
   "source": [
    "The loading from drive part worked before, but now that i have more embeddings because i changed embedding size, it doesn't work cuz appearently\n",
    "\n",
    "Well the probelem is not exactly size but faiss, i am trying to save faiss indices, and faiss is c/cpp backend not python. It contains 'non python' thing which pickle is not so much comfortable be, for instance, pointers allocated outside python heap or native memory buffers, so things break.\n",
    "\n",
    "So the only option is to rebuild index every iteration using th eembeddings.npy made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zf-kVpAe7Msc"
   },
   "source": [
    "well faiss expects query to be list and accordingly returns similarity score and index which are both 2d list, 1 dimention for each query and second for each vector returned for query. Thus we loop over both for retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U97E-MG5gQGH",
    "outputId": "18a888b5-c67c-4978-cb18-dfc9d31edc09"
   },
   "outputs": [],
   "source": [
    "def search_info(query, model, metadata, top_k=5):\n",
    "    # first use the same sentance embedding model to make query database compatible\n",
    "    query_vector = model.encode(query, normalize_embeddings=True, convert_to_numpy=True)\n",
    "\n",
    "    sim_1, idx_1 = index_bns.search(query_vector, top_k)\n",
    "    sim_2, idx_2 = index_apex.search(query_vector, top_k)\n",
    "    sim_3, idx_3 = index_high.search(query_vector, top_k )\n",
    "\n",
    "    chunks_bns, chunks_apex, chunks_high = metadata\n",
    "    print(len(query))\n",
    "    for q in range(len(query)):\n",
    "        for i in range(top_k):\n",
    "            print(f'{i+1}th most similar to query in bns with similarity :: {sim_1[q][i]} is :: \\n{chunks_bns[idx_1[q][i]]}')\n",
    "            print(f'{i+1}th most similar to query in apex with similarity :: {sim_2[q][i]} is :: \\n{chunks_apex[idx_2[q][i]]}')\n",
    "            print(f'{i+1}th most similar to query in high with similarity :: {sim_3[q][i]} is :: \\n{chunks_high[idx_3[q][i]]}')\n",
    "\n",
    "\n",
    "query = [\n",
    "    \"Can a criminal conviction be sustained solely on the testimony of a single eyewitness?\",\n",
    "    \"Is delay in filing an FIR a sufficient ground to discard the prosecution case?\"\n",
    "    ]\n",
    "\n",
    "metadata = (chunks_bns, chunks_apex, chunks_high)\n",
    "search_info(query, model, metadata, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GkQUD82AIEO1"
   },
   "outputs": [],
   "source": [
    "# Confirming about whether faiss and chunked_* are same data\n",
    "print(f'length of chunks_bns is {len(chunks_bns)} :: embedded = {len(embedding_bns)}')\n",
    "print(f'length of chunks_bns is {len(chunks_apex)} :: embedded = {len(embedding_apex)}')\n",
    "print(f'length of chunks_bns is {len(chunks_high)} :: embedded = {len(embedding_high)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ltBTLWGrGdmQ"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/content/chunk_text.pkl', 'wb') as f:\n",
    "    pickle.dump(metadata, f) # no need for encoding(s) when working in binary\n",
    "!cp /content/chunk_text.pkl /content/drive/MyDrive/colab_stuff/court_hearings_rag/chunk_text.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QB0Q5Dq77dRH"
   },
   "source": [
    "The faiss part is repeated in next one as well, since i need to build faiss again cuz c++. But i am taking metadata, which is not metadata but that is thae name coming to my mind everytime i think of it so i name it metadata. I save it to file and use it in next notebook, this won't break, as metadata is made of all python elements so pickle will pickle it."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
